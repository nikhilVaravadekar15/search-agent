import asyncio
import json
import time
from datetime import date
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple
from uuid import UUID

from fastapi import HTTPException, Request
from langchain.agents import AgentState, create_agent
from langchain.agents.middleware import (
    SummarizationMiddleware,
    TodoListMiddleware,
    ToolCallLimitMiddleware,
)
from langchain.messages import AIMessageChunk, ToolMessage
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph.state import CompiledStateGraph
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

from src.commonlib.config import settings
from src.commonlib.constants import (
    LLM_RESPONSE_GENERATION_ERROR,
    LLM_RESPONSE_STREAMING_ERROR,
)
from src.commonlib.logger import search_logger
from src.search import crud
from src.search import types as search_types
from src.search.agents.prompts import RESEARCHER_INSTRUCTIONS
from src.search.agents.tools import internet_search, think_tool


class AgentManager:
    """Manages agent lifecycle and streaming."""

    def __init__(self):
        self._search_agent = None
        self._checkpointer = InMemorySaver(
            serde=JsonPlusSerializer(pickle_fallback=True)
        )
        self._lock = asyncio.Lock()
        self.running_agent_tasks: Dict[str, asyncio.Task] = {}

        self.llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            base_url=settings.MODEL_URL,
            api_key=settings.API_KEY,
            temperature=settings.MODEL_TEMPERATURE,
            profile={
                "max_input_tokens": settings.MAX_INPUT_TOKENS,
                "tool_calling": True,
                "structured_output": False,
            },
        )

    def _create_search_agent(self) -> CompiledStateGraph[AgentState, Any]:
        """
        Create agent for search queries (with full tool calling support).

        This agent has access to all legal search tools and must use them
        to answer legal questions.

        Return:
            agent (CompiledStateGraph)
        """
        tools = [internet_search, think_tool]

        system_prompt = RESEARCHER_INSTRUCTIONS.format(date=date.today())
        agent = create_agent(
            name=settings.AGENT_NAME,
            model=self.llm,
            tools=tools,
            system_prompt=system_prompt,
            debug=True,
            checkpointer=self._checkpointer,
            middleware=[
                TodoListMiddleware(),
                ToolCallLimitMiddleware(run_limit=settings.TOOL_CALLING_LIMIT),
                SummarizationMiddleware(
                    model=self.llm,
                    trigger=("fraction", settings.SUMMERIZATION_MIDDLEWARE_LIMIT),
                ),
            ],
        )

        return agent

    async def get_search_agent(self) -> CompiledStateGraph[AgentState, Any]:
        """
        Thread-safe lazy initialization of legal agent.

        Returns:
            agent (CompiledStateGraph[AgentState, Any])
        """
        if self._search_agent is None:
            async with self._lock:
                if self._search_agent is None:
                    self._search_agent = self._create_search_agent()
                    search_logger.info("Creating search agent")
                else:
                    search_logger.info("search agent already created by another task")
        return self._search_agent

    def sse_event(self, data: search_types.SseEvent):
        """
        Format SSE event.

        Args:
            data (search_types.SseEvent):
                mode (SseMode)
                message (str): llm generated message
                context (Optional[Dict]): Optional context such as tool call arguments generated by the agent or ToolMessage
                thread_id (UUID): current conversation thread
                track_id (UUID): track and stop stream
                message_id (Optional[UUID]): previous message
        Returns:
            str
        """
        return f"data: {data.model_dump_json()}\n\n"

    async def stream_events(
        self,
        thread_id: UUID,
        track_id: UUID,
        query: str,
        message_id: Optional[UUID],
        follow_context: Optional[search_types.FollowContext],
    ) -> AsyncGenerator[str, None]:
        """
        Run the streaming job/message.
        Args:
            thread_id (UUID): reference to conversation-thread model
            track_id (UUID): message to save conversation-thread details against
            query (str): user query
            message_id (Optional[UUID]):  current ai message id (parent id for next uer message)
            follow_context (Optional[search_types.FollowContext]): follow up message context
        """
        final_output = ""
        error_message = ""
        sources: List[search_types.Source] = []

        try:
            search_logger.info(
                f"starting search for thread_id={thread_id}, track_id={track_id}"
            )

            # Classify the query intent
            yield search_types.SseEvent(
                mode=search_types.SseMode.THINKING,
                message="Analyzing user's request",
                thread_id=thread_id,
                track_id=track_id,
                message_id=message_id,
            )

            agent = await self.get_search_agent()
            search_logger.info(
                f"Created {agent.name} for thread_id={thread_id}, track_id={track_id} & Query={query}"
            )

            # messages: List[BaseMessage] = []
            # if follow_context:
            #     system_hint = None
            #     if follow_context.type == search_types.FollowType.FOLLOW_UP:
            #         system_hint = (
            #             "The user is asking a question about the following "
            #             f"selected text: {follow_context.text}\n"
            #             "Answer with this context in mind."
            #         )
            #     elif follow_context.type == search_types.FollowType.REGENERATE:
            #         system_hint = (
            #             "The user requested a regenerated answer. "
            #             "Provide an alternative explanation."
            #         )
            #     if system_hint:
            #         messages.append(SystemMessage(content=system_hint))

            # # append actual user query
            # messages.append()
            # search_logger.info(messages)

            try:
                async for chunk in agent.astream(
                    input={"messages": [HumanMessage(content=query)]},
                    stream_mode=["updates", "messages", "custom"],
                    config={"configurable": {"thread_id": thread_id}},
                    context={
                        "track_id": track_id,
                        "thread_id": thread_id,
                        "follow_context": (
                            follow_context.model_dump(mode="json")
                            if follow_context
                            else None
                        ),
                    },
                ):
                    if not isinstance(chunk, Tuple):
                        continue

                    chunk_type, content = chunk
                    if chunk_type == "messages":
                        message_chunk, metadata = content

                        if isinstance(message_chunk, AIMessageChunk):
                            if message_chunk.content and (
                                "langgraph_node" in metadata
                                and metadata["langgraph_node"] == "model"
                            ):
                                token = message_chunk.content
                                final_output += token
                                yield search_types.SseEvent(
                                    mode=search_types.SseMode.RESPONSE,
                                    message=token,
                                    thread_id=thread_id,
                                    track_id=track_id,
                                    message_id=message_id,
                                )

                            if message_chunk.tool_calls:
                                for tool_call in message_chunk.tool_calls:
                                    msg = f"Calling {tool_call['name']} tool"
                                    tool_info = {
                                        "args": tool_call["args"],
                                    }
                                    yield search_types.SseEvent(
                                        mode=search_types.SseMode.THINKING,
                                        message=msg,
                                        context=tool_info,
                                        thread_id=thread_id,
                                        track_id=track_id,
                                        message_id=message_id,
                                    )
                        elif isinstance(message_chunk, ToolMessage):
                            tool_name = getattr(message_chunk, "name", "unknown")
                            tool_status = getattr(message_chunk, "status", "success")

                            if tool_status == "success":
                                tool_sources = message_chunk.additional_kwargs
                                if isinstance(metadata, dict):
                                    sources.extend(tool_sources.get("sources", []))
                                msg = f"Successfully retrieved data using {tool_name} tool"
                            else:
                                msg = f"Failed to retrieve data using {tool_name} tool"

                            search_logger.info(
                                f"{msg} for thread_id={thread_id}, track_id={track_id}"
                            )
                            yield search_types.SseEvent(
                                mode=search_types.SseMode.THINKING,
                                message=msg,
                                thread_id=thread_id,
                                track_id=track_id,
                                message_id=message_id,
                            )
                        else:
                            continue
                    elif chunk_type == "custom":
                        yield search_types.SseEvent(
                            mode=search_types.SseMode.THINKING,
                            message=content,
                            thread_id=thread_id,
                            track_id=track_id,
                            message_id=message_id,
                        )
                    else:
                        continue

            except Exception as e:
                search_logger.error(
                    f"Error {e.__class__.__name__}: thread_id={thread_id}, track_id={track_id} & {str(e)}",
                    exc_info=True,
                )
                yield search_types.SseEvent(
                    mode=search_types.SseMode.ERROR,
                    message=LLM_RESPONSE_GENERATION_ERROR,
                    thread_id=thread_id,
                    track_id=track_id,
                    message_id=message_id,
                )

            if not final_output and not error_message:
                final_output += LLM_RESPONSE_STREAMING_ERROR
                yield search_types.SseEvent(
                    mode=search_types.SseMode.RESPONSE,
                    message=f"{LLM_RESPONSE_STREAMING_ERROR}",
                    thread_id=thread_id,
                    track_id=track_id,
                    message_id=message_id,
                )

            return

        except HTTPException as e:
            # API-level controlled errors
            search_logger.error(
                f"HTTPException in message streaming for thread_id={thread_id}, track_id={track_id} & {str(e)}",
                exc_info=True,
            )
            raise e
        except asyncio.CancelledError as ce:
            # Client disconnected (normal)
            search_logger.error(
                f"Client disconnected during stream, thread_id={thread_id}, track_id={track_id} & {str(ce)}",
                exc_info=True,
            )
            yield search_types.SseEvent(
                mode=search_types.SseMode.ERROR,
                message=LLM_RESPONSE_STREAMING_ERROR,
                thread_id=thread_id,
                track_id=track_id,
                message_id=message_id,
            )
            raise ce
        except Exception as e:
            # Unknown / critical error
            search_logger.error(
                f"Unhandled exception in message streaming for thread_id={thread_id}, track_id={track_id} & {str(e)}",
                exc_info=True,
            )

            yield search_types.SseEvent(
                mode=search_types.SseMode.ERROR,
                message=f"Error: {str(e)}",
                thread_id=thread_id,
                track_id=track_id,
                message_id=message_id,
            )
        finally:
            yield search_types.SseEvent(
                mode=search_types.SseMode.METADATA,
                message=json.dumps(
                    [source.model_dump(mode="json") for source in sources]
                ),
                thread_id=thread_id,
                track_id=track_id,
                message_id=message_id,
            )
            yield search_types.SseEvent(
                mode=search_types.SseMode.RESPONSE,
                message="[DONE]",
                thread_id=thread_id,
                track_id=track_id,
                message_id=message_id,
            )

    async def run(
        self,
        request: Request,
        query: str,
        thread_id: UUID,
        track_id: UUID,
        message_id: Optional[UUID],
        follow_context: Optional[search_types.FollowContext],
    ):
        """
        Execute agent in background task and stream responses with cancellation support.
        flow:
            - Main coroutine: Consumes from queue and yields SSE events to client
            - Background task (agent_worker): Runs agent and pushes chunks to queue
            - Queue: Thread-safe communication channel between producer and consumer
            - Cancellation: Gracefully stops agent when client disconnects

        Args:
            request (Request): FastAPI request object to check client connection status
            query (str): User's natural language query to process
            thread_id (UUID): Unique identifier for the conversation thread
            track_id (UUID): Unique identifier to track and stop execution (user message id)
            message_id (Optional[UUID]): Unique identifier of the previous parent message (ai message id)
            follow_context: (Optional[search_types.FollowContext]): follow up context

        Yields:
            str: SSE-formatted events containing agent responses, including:
                - Planning events (todo list creation)
                - Tool execution progress messages
                - Incremental response chunks
                - Metadata (sources, citations)
                - Error messages

        Raises:
            HTTPException: If agent execution fails critically
            asyncio.CancelledError: If client disconnects
        """
        agent_task = None
        track_id_str = str(track_id)
        chunk_queue: asyncio.Queue[
            Tuple[search_types.SseMessageType, search_types.SseEvent]
        ] = asyncio.Queue(maxsize=100)

        async def agent_worker():
            """Background worker that runs the agent and handles cleanup."""
            try:
                agent_generator = self.stream_events(
                    thread_id=thread_id,
                    track_id=track_id_str,
                    query=query,
                    message_id=message_id,
                    follow_context=follow_context,
                )

                async for chunk in agent_generator:
                    await chunk_queue.put((search_types.SseMessageType.CHUNK, chunk))

                # Signal completion
                await chunk_queue.put((search_types.SseMessageType.DONE, None))
                search_logger.info(
                    f"Agent completed successfully for thread_id={thread_id}, track_id={track_id}"
                )

            except asyncio.CancelledError:
                search_logger.info(
                    f"Agent cancelled for thread_id={thread_id}, track_id={track_id}, performing cleanup"
                )
                await chunk_queue.put(
                    (
                        search_types.SseMessageType.CANCELLED,
                        LLM_RESPONSE_STREAMING_ERROR,
                    )
                )
                raise

            except Exception as e:
                search_logger.error(
                    f"Agent error for thread_id={thread_id} track_id={track_id}: {e}",
                    exc_info=True,
                )
                await chunk_queue.put((search_types.SseMessageType.ERROR, str(e)))
                raise
            finally:
                self.running_agent_tasks.pop(track_id_str, None)

        try:
            final_output = ""
            error_message = ""
            streaming = True
            sources: List[search_types.Source] = []

            # Create and track the background task
            agent_task = asyncio.create_task(agent_worker())
            self.running_agent_tasks[track_id_str] = agent_task

            # Stream chunks to client while monitoring disconnection
            while streaming:
                # Check for disconnection
                if await request.is_disconnected():
                    search_logger.info(
                        f"Client disconnected for thread_id={thread_id}, track_id={track_id}, cancelling agent"
                    )

                    # Cancel the agent task (cleanup will happen in except block)
                    if agent_task and not agent_task.done():
                        agent_task.cancel()
                        try:
                            await agent_task
                        except asyncio.CancelledError:
                            search_logger.info(
                                f"Agent task cancelled for track_id={track_id}"
                            )

                    streaming = False
                    break

                # Get next chunk with timeout
                try:
                    chunk = await asyncio.wait_for(chunk_queue.get(), timeout=0.5)
                    chunk_type, chunk_content = chunk
                    if chunk_type == search_types.SseMessageType.CHUNK:
                        # Accumulate output/sources if needed
                        if chunk_content.mode == search_types.SseMode.RESPONSE:
                            if chunk_content.message != "[DONE]":
                                final_output += chunk_content.message
                        elif chunk_content.mode == search_types.SseMode.METADATA:
                            try:
                                metadata = json.loads(chunk_content.message)
                                sources.extend(metadata)
                            except Exception:
                                search_logger.warning("failed to parse sources")
                                sources.extend([])
                        elif chunk_content.mode == search_types.SseMode.ERROR:
                            error_message = chunk_content.message
                            search_logger.info(error_message)

                        yield self.sse_event(data=chunk_content)

                    elif chunk_type == search_types.SseMessageType.DONE:
                        search_logger.info(
                            f"Agent stream ended for thread_id={thread_id}, track_id={track_id}"
                        )
                        streaming = False
                        break
                    elif (
                        chunk_type == search_types.SseMessageType.ERROR
                        or chunk_type == search_types.SseMessageType.CANCELLED
                    ):
                        error_message = chunk_content
                        break
                except asyncio.TimeoutError:
                    # No chunk available, loop back to check disconnection
                    continue

            search_logger.info(f"final output: {final_output}")
            search_logger.info(f"error: {error_message if error_message else None}")
            search_logger.info(f"sources: {sources if sources else None}")
            # single db call to updated status based on the error or final_message
            await crud.update_aimessage_in_thread(
                thread_id=thread_id,
                message_id=message_id,
                content=final_output,
                error_message=error_message if error_message else None,
                sources=sources if sources else None,
            )
        except Exception as e:
            search_logger.error(
                f"Error in stream consumer for thread_id={thread_id}, track_id={track_id}: {str(e)}",
                exc_info=True,
            )
            if agent_task and not agent_task.done():
                agent_task.cancel()
                try:
                    await agent_task
                except asyncio.CancelledError:
                    raise
            raise
        finally:
            # 1. Cleanup task reference
            self.running_agent_tasks.pop(track_id_str, None)

            # 2. Ensure agent task is done
            if agent_task and not agent_task.done():
                agent_task.cancel()
                try:
                    await asyncio.wait_for(agent_task, timeout=1.0)
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass

            # 3. Explicitly delete reference (helps GC, but not strictly necessary)
            del chunk_queue
            search_logger.debug(
                f"Cleaned up all resources for thread_id={thread_id} track_id={track_id}"
            )

    async def stop_stream_message(self, track_id: UUID) -> bool:
        """
        Cancel the running search job/message

        Args:
            track_id (UUID): tracking message id
        Returns:
            bool
        """
        try:
            track_id_str = str(track_id)
            if track_id_str not in self.running_agent_tasks:
                return False

            task = self.running_agent_tasks.get(track_id_str)
            if task is None or task.done():
                return False

            # cancel and delete the track_id entry
            task.cancel()
            self.running_agent_tasks.pop(track_id_str, None)
            try:
                await task
            except asyncio.CancelledError:
                pass

            return True
        except Exception:
            raise
