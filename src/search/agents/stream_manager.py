import asyncio
import json
import threading
from typing import AsyncGenerator, Dict, List, Optional, Tuple
from uuid import UUID

from fastapi import HTTPException, Request
from langchain.messages import AIMessageChunk, ToolMessage
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage

from src.commonlib.constants import (
    LLM_RESPONSE_GENERATION_ERROR,
    LLM_RESPONSE_STREAMING_ERROR,
)
from src.commonlib.logger import search_logger
from src.search import crud
from src.search import types as search_types
from src.search.agents.agent_manager import AgentManager


class StreamManager:
    """Manages stream lifecycle and events"""

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, "_initialized") or not self._initialized:
            self.agent_manager = AgentManager()
            self.running_agent_tasks: Dict[str, asyncio.Task] = {}
            self._initialized = True

    def sse_event(self, data: search_types.SseEvent):
        """
        Format SSE event.

        Args:
            data (search_types.SseEvent):
                mode (SseMode)
                message (str): llm generated message
                context (Optional[Dict]): Optional context such as tool call arguments generated by the agent or ToolMessage
                thread_id (UUID): current conversation thread
                track_id (UUID): track and stop stream
                aim_id (Optional[UUID]): previous message
        Returns:
            str
        """
        return f"data: {data.model_dump_json()}\n\n"

    async def stream_events(
        self,
        query: str,
        thread_id: UUID,
        track_id: UUID,
        um_id: Optional[UUID],
        aim_id: Optional[UUID],
        enable_search: bool,
        context: search_types.Flow,
    ) -> AsyncGenerator[str, None]:
        """
        Run the streaming job/message.
        Args:
            query (str): user query
            thread_id (UUID): reference to conversation-thread model
            track_id (UUID): message to save conversation-thread details against
            um_id (Optional[UUID]): current user message id
            aim_id (Optional[UUID]): current ai message id (parent id for next uer message)
            enable_search (bool): Whether search is enabled
            context: (search_types.Flow): Actual follow up context
        """
        final_output = ""
        error_message = ""
        sources: List[search_types.Source] = []

        try:
            agent_name: search_types.AGENT_TYPES = "question_answering"
            if enable_search:  # if search is enabled, use the search agent
                agent_name = "search"

            agent = await self.agent_manager.get_agent(name=agent_name)
            search_logger.info(
                f"Created {agent.name} for thread_id={thread_id}, track_id={track_id} & Query={query}"
            )

            # start of the stream
            yield search_types.SseEvent(
                mode=search_types.SseMode.THINKING,
                message="Analyzing user's request",
                thread_id=thread_id,
                track_id=track_id,
                um_id=um_id,
                aim_id=aim_id,
            )

            messages: List[BaseMessage] = []
            if context.type.value == search_types.FlowType.FOLLOW_UP.value:
                system_hint = (
                    "The user is asking a question about the following "
                    f"selected text: {context.text}\n"
                    "Answer with this context in mind."
                )
                messages.append(SystemMessage(content=system_hint))
            elif context.type.value == search_types.FlowType.REGENERATE.value:
                system_hint = (
                    "The user requested a regenerated answer. "
                    "Provide an alternative explanation."
                )
                messages.append(SystemMessage(content=system_hint))

            # append actual user query
            messages.append(HumanMessage(content=query))
            search_logger.info(messages)

            try:
                async for chunk in agent.astream(
                    input={"messages": messages},
                    stream_mode=["updates", "messages", "custom"],
                    config={"configurable": {"thread_id": thread_id}},
                    context={
                        "track_id": track_id,
                        "thread_id": thread_id,
                        "context": context.model_dump(),
                    },
                ):
                    if not isinstance(chunk, Tuple):
                        continue

                    chunk_type, content = chunk
                    if chunk_type == "messages":
                        message_chunk, metadata = content

                        if isinstance(message_chunk, AIMessageChunk):
                            if message_chunk.content and (
                                "langgraph_node" in metadata
                                and metadata["langgraph_node"] == "model"
                            ):
                                token = message_chunk.content
                                final_output += token
                                yield search_types.SseEvent(
                                    mode=search_types.SseMode.RESPONSE,
                                    message=token,
                                    thread_id=thread_id,
                                    track_id=track_id,
                                    um_id=um_id,
                                    aim_id=aim_id,
                                )

                            if message_chunk.tool_calls:
                                for tool_call in message_chunk.tool_calls:
                                    msg = f"Calling {tool_call['name']} tool"
                                    tool_info = {
                                        "args": tool_call["args"],
                                    }
                                    yield search_types.SseEvent(
                                        mode=search_types.SseMode.THINKING,
                                        message=msg,
                                        meta=tool_info,
                                        thread_id=thread_id,
                                        track_id=track_id,
                                        um_id=um_id,
                                        aim_id=aim_id,
                                    )
                        elif isinstance(message_chunk, ToolMessage):
                            tool_name = getattr(message_chunk, "name", "unknown")
                            tool_status = getattr(message_chunk, "status", "success")

                            if tool_status == "success":
                                tool_sources = message_chunk.additional_kwargs
                                if isinstance(metadata, dict):
                                    sources.extend(tool_sources.get("sources", []))
                                msg = f"Successfully retrieved data using {tool_name} tool"
                            else:
                                msg = f"Failed to retrieve data using {tool_name} tool"

                            search_logger.info(
                                f"{msg} for thread_id={thread_id}, track_id={track_id}"
                            )
                            yield search_types.SseEvent(
                                mode=search_types.SseMode.THINKING,
                                message=msg,
                                thread_id=thread_id,
                                track_id=track_id,
                                um_id=um_id,
                                aim_id=aim_id,
                            )
                        else:
                            continue
                    elif chunk_type == "custom":
                        yield search_types.SseEvent(
                            mode=search_types.SseMode.THINKING,
                            message=content,
                            thread_id=thread_id,
                            track_id=track_id,
                            um_id=um_id,
                            aim_id=aim_id,
                        )
                    else:
                        continue

            except Exception as e:
                search_logger.error(
                    f"Error {e.__class__.__name__}: thread_id={thread_id}, track_id={track_id} & {str(e)}",
                    exc_info=True,
                )
                yield search_types.SseEvent(
                    mode=search_types.SseMode.ERROR,
                    message=LLM_RESPONSE_GENERATION_ERROR,
                    thread_id=thread_id,
                    track_id=track_id,
                    um_id=um_id,
                    aim_id=aim_id,
                )

            if not final_output and not error_message:
                final_output += LLM_RESPONSE_STREAMING_ERROR
                yield search_types.SseEvent(
                    mode=search_types.SseMode.RESPONSE,
                    message=f"{LLM_RESPONSE_STREAMING_ERROR}",
                    thread_id=thread_id,
                    track_id=track_id,
                    um_id=um_id,
                    aim_id=aim_id,
                )

            return

        except HTTPException as e:
            # API-level controlled errors
            search_logger.error(
                f"HTTPException in message streaming for thread_id={thread_id}, track_id={track_id} & {str(e)}",
                exc_info=True,
            )
            raise e
        except asyncio.CancelledError as ce:
            # Client disconnected (normal)
            search_logger.error(
                f"Client disconnected during stream, thread_id={thread_id}, track_id={track_id} & {str(ce)}",
                exc_info=True,
            )
            yield search_types.SseEvent(
                mode=search_types.SseMode.ERROR,
                message=LLM_RESPONSE_STREAMING_ERROR,
                thread_id=thread_id,
                track_id=track_id,
                um_id=um_id,
                aim_id=aim_id,
            )
            raise ce
        except Exception as e:
            # Unknown / critical error
            search_logger.error(
                f"Unhandled exception in message streaming for thread_id={thread_id}, track_id={track_id} & {str(e)}",
                exc_info=True,
            )

            yield search_types.SseEvent(
                mode=search_types.SseMode.ERROR,
                message=f"Error: {str(e)}",
                thread_id=thread_id,
                track_id=track_id,
                um_id=um_id,
                aim_id=aim_id,
            )
        finally:
            yield search_types.SseEvent(
                mode=search_types.SseMode.METADATA,
                message=json.dumps(
                    [source.model_dump(mode="json") for source in sources]
                ),
                thread_id=thread_id,
                track_id=track_id,
                um_id=um_id,
                aim_id=aim_id,
            )
            yield search_types.SseEvent(
                mode=search_types.SseMode.RESPONSE,
                message="[DONE]",
                thread_id=thread_id,
                track_id=track_id,
                um_id=um_id,
                aim_id=aim_id,
            )

    async def run(
        self,
        request: Request,
        query: str,
        thread_id: UUID,
        track_id: UUID,
        um_id: UUID,
        aim_id: Optional[UUID],
        enable_search: bool,
        context: search_types.Flow,
    ):
        """
        Execute agent in background task and stream responses with cancellation support.
        flow:
            - Main coroutine: Consumes from queue and yields SSE events to client
            - Background task (agent_worker): Runs agent and pushes chunks to queue
            - Queue: Thread-safe communication channel between producer and consumer
            - Cancellation: Gracefully stops agent when client disconnects

        Args:
            request (Request): FastAPI request object to check client connection status
            query (str): User's natural language query to process
            thread_id (UUID): Unique identifier for the conversation thread
            track_id (UUID): Unique identifier to track and stop execution (user message id)
            um_id (UUID): Unique identifier of user message id
            aim_id (Optional[UUID]): Unique identifier of the previous parent message (ai message id)
            enable_search (bool): Whether search is enabled
            context: (search_types.Flow): Actual follow up context

        Yields:
            str: SSE-formatted events containing agent responses, including:
                - Planning events (todo list creation)
                - Tool execution progress messages
                - Incremental response chunks
                - Metadata (sources, citations)
                - Error messages

        Raises:
            HTTPException: If agent execution fails critically
            asyncio.CancelledError: If client disconnects
        """
        agent_task = None
        track_id_str = str(track_id)
        chunk_queue: asyncio.Queue[
            Tuple[search_types.SseMessageType, search_types.SseEvent]
        ] = asyncio.Queue(maxsize=100)

        async def agent_worker():
            """Background worker that runs the agent and handles cleanup."""
            try:
                agent_generator = self.stream_events(
                    query=query,
                    thread_id=thread_id,
                    track_id=track_id_str,
                    um_id=um_id,
                    aim_id=aim_id,
                    enable_search=enable_search,
                    context=context,
                )

                async for chunk in agent_generator:
                    await chunk_queue.put((search_types.SseMessageType.CHUNK, chunk))

                # Signal completion
                await chunk_queue.put((search_types.SseMessageType.DONE, None))
                search_logger.info(
                    f"Agent completed successfully for thread_id={thread_id}, track_id={track_id}"
                )

            except asyncio.CancelledError:
                search_logger.info(
                    f"Agent cancelled for thread_id={thread_id}, track_id={track_id}, performing cleanup"
                )
                await chunk_queue.put(
                    (
                        search_types.SseMessageType.CANCELLED,
                        LLM_RESPONSE_STREAMING_ERROR,
                    )
                )
                raise

            except Exception as e:
                search_logger.error(
                    f"Agent error for thread_id={thread_id} track_id={track_id}: {e}",
                    exc_info=True,
                )
                await chunk_queue.put((search_types.SseMessageType.ERROR, str(e)))
                raise
            finally:
                self.running_agent_tasks.pop(track_id_str, None)

        try:
            final_output = ""
            error_message = ""
            streaming = True
            sources: List[search_types.Source] = []

            # Create and track the background task
            agent_task = asyncio.create_task(agent_worker())
            self.running_agent_tasks[track_id_str] = agent_task

            # Stream chunks to client while monitoring disconnection
            while streaming:
                # Check for disconnection
                if await request.is_disconnected():
                    search_logger.info(
                        f"Client disconnected for thread_id={thread_id}, track_id={track_id}, cancelling agent"
                    )

                    # Cancel the agent task (cleanup will happen in except block)
                    if agent_task and not agent_task.done():
                        agent_task.cancel()
                        try:
                            await agent_task
                        except asyncio.CancelledError:
                            search_logger.info(
                                f"Agent task cancelled for track_id={track_id}"
                            )

                    streaming = False
                    break

                # Get next chunk with timeout
                try:
                    chunk = await asyncio.wait_for(chunk_queue.get(), timeout=0.5)
                    chunk_type, chunk_content = chunk
                    if chunk_type == search_types.SseMessageType.CHUNK:
                        # Accumulate output/sources if needed
                        if chunk_content.mode == search_types.SseMode.RESPONSE:
                            if chunk_content.message != "[DONE]":
                                final_output += chunk_content.message
                        elif chunk_content.mode == search_types.SseMode.METADATA:
                            try:
                                metadata = json.loads(chunk_content.message)
                                sources.extend(metadata)
                            except Exception:
                                search_logger.warning("failed to parse sources")
                                sources.extend([])
                        elif chunk_content.mode == search_types.SseMode.ERROR:
                            error_message = chunk_content.message
                            search_logger.info(error_message)

                        yield self.sse_event(data=chunk_content)

                    elif chunk_type == search_types.SseMessageType.DONE:
                        search_logger.info(
                            f"Agent stream ended for thread_id={thread_id}, track_id={track_id}"
                        )
                        streaming = False
                        break
                    elif (
                        chunk_type == search_types.SseMessageType.ERROR
                        or chunk_type == search_types.SseMessageType.CANCELLED
                    ):
                        error_message = chunk_content
                        break
                except asyncio.TimeoutError:
                    # No chunk available, loop back to check disconnection
                    continue

            search_logger.info(f"final output: {final_output}")
            search_logger.info(f"error: {error_message if error_message else None}")
            search_logger.info(f"sources: {sources if sources else None}")
            # single db call to updated status based on the error or final_message
            await crud.update_aimessage_in_thread(
                thread_id=thread_id,
                aim_id=aim_id,
                content=final_output,
                error_message=error_message if error_message else None,
                sources=sources if sources else None,
            )
        except Exception as e:
            search_logger.error(
                f"Error in stream consumer for thread_id={thread_id}, track_id={track_id}: {str(e)}",
                exc_info=True,
            )
            if agent_task and not agent_task.done():
                agent_task.cancel()
                try:
                    await agent_task
                except asyncio.CancelledError:
                    raise
            raise
        finally:
            # 1. Cleanup task reference
            self.running_agent_tasks.pop(track_id_str, None)

            # 2. Ensure agent task is done
            if agent_task and not agent_task.done():
                agent_task.cancel()
                try:
                    await asyncio.wait_for(agent_task, timeout=1.0)
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass

            # 3. Explicitly delete reference (helps GC, but not strictly necessary)
            del chunk_queue
            search_logger.debug(
                f"Cleaned up all resources for thread_id={thread_id} track_id={track_id}"
            )

    async def stop_stream_message(self, track_id: UUID) -> bool:
        """
        Cancel the running search job/message

        Args:
            track_id (UUID): tracking message id
        Returns:
            bool
        """
        try:
            track_id_str = str(track_id)
            if track_id_str not in self.running_agent_tasks:
                return False

            task = self.running_agent_tasks.get(track_id_str)
            if task is None or task.done():
                return False

            # cancel and delete the track_id entry
            task.cancel()
            self.running_agent_tasks.pop(track_id_str, None)
            try:
                await task
            except asyncio.CancelledError:
                pass

            return True
        except Exception:
            raise
